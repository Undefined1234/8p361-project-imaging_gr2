{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder with Convolutional Neural Network\n",
    "This notebook contains the implementation of an autoencoder for augmentation of the histopathology dataset. The augmented data is used to train CNN networks to see if the accuracy and AUC are higher compared to the baseline CNN model trained on the normal dataset. First, the required libraries are imported and the size of the images in the PCAM dataset is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the functions and classes from main_util.py\n",
    "from main_util import get_pcam_generators\n",
    "from main_util import Model_architecture\n",
    "from main_util import Model_transform\n",
    "\n",
    "# Standard libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Modelcheckpoint and tensorboard callbacks\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# ROC curve analysis\n",
    "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
    "\n",
    "# the size of the images in the PCAM dataset\n",
    "IMAGE_SIZE = 96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating data generators\n",
    "\n",
    "The PatchCAMELYON dataset is too big to fit in the working memory of most personal computers. This is why, we need to define some functions that will read the image data batch by batch, so only a single batch of images needs to be stored in memory at one time point. We can use the handy ImageDataGenerator function from the Keras API to do this. Note that the generators are defined within the function `get_pcam_generators` that returns them as output arguments. This function will later be called from the main code body. The function is located in `main_util.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before executing the code block below, do not forget to change the path where the PatchCAMELYON dataset is located (that is, the location of the folder that contains `train+val` that you previously downloaded and unpacked).\n",
    "\n",
    "If everything is correct, the following output will be printed on screen after executing the code block:\n",
    "\n",
    "`Found 144000 images belonging to 2 classes.`\n",
    "\n",
    "`Found 16000 images belonging to 2 classes.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Het systeem kan het opgegeven pad niet vinden: '../data\\\\train+val\\\\train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m train_gen, val_gen \u001b[38;5;241m=\u001b[39m \u001b[43mget_pcam_generators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mtrain_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbinary\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\20212059\\OneDrive - TU Eindhoven\\Documents\\School\\3e jaar\\DBL_AIMIA\\8p361-project-imaging_gr2\\main_project\\main_util.py:30\u001b[0m, in \u001b[0;36mget_pcam_generators\u001b[1;34m(base_dir, train_batch_size, val_batch_size, class_mode, prep_function)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Instantiate data generators with correct preprocessing function\u001b[39;00m\n\u001b[0;32m     28\u001b[0m datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(rescale\u001b[38;5;241m=\u001b[39mRESCALING_FACTOR, preprocessing_function\u001b[38;5;241m=\u001b[39mprep_function)\n\u001b[1;32m---> 30\u001b[0m train_gen \u001b[38;5;241m=\u001b[39m \u001b[43mdatagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow_from_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m val_gen \u001b[38;5;241m=\u001b[39m datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(VALID_PATH,\n\u001b[0;32m     37\u001b[0m                                       target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m96\u001b[39m,\u001b[38;5;241m96\u001b[39m),\n\u001b[0;32m     38\u001b[0m                                       batch_size\u001b[38;5;241m=\u001b[39mval_batch_size,\n\u001b[0;32m     39\u001b[0m                                       class_mode\u001b[38;5;241m=\u001b[39mclass_mode,\n\u001b[0;32m     40\u001b[0m                                       shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_gen, val_gen\n",
      "File \u001b[1;32mc:\\Users\\20212059\\.conda\\envs\\8p361\\lib\\site-packages\\keras\\preprocessing\\image.py:1650\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_directory\u001b[1;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflow_from_directory\u001b[39m(\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1566\u001b[0m     directory,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1580\u001b[0m     keep_aspect_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1581\u001b[0m ):\n\u001b[0;32m   1582\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Takes the path to a directory & generates batches of augmented data.\u001b[39;00m\n\u001b[0;32m   1583\u001b[0m \n\u001b[0;32m   1584\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1648\u001b[0m \u001b[38;5;124;03m            and `y` is a numpy array of corresponding labels.\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDirectoryIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1652\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1662\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_to_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\20212059\\.conda\\envs\\8p361\\lib\\site-packages\\keras\\preprocessing\\image.py:563\u001b[0m, in \u001b[0;36mDirectoryIterator.__init__\u001b[1;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[0;32m    562\u001b[0m     classes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[0;32m    564\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[0;32m    565\u001b[0m             classes\u001b[38;5;241m.\u001b[39mappend(subdir)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Het systeem kan het opgegeven pad niet vinden: '../data\\\\train+val\\\\train'"
     ]
    }
   ],
   "source": [
    "path = \"../data\"\n",
    "train_gen, val_gen = get_pcam_generators(path, \n",
    "                                         train_batch_size=16, \n",
    "                                         val_batch_size=16,\n",
    "                                         class_mode=\"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the baseline CNN model on regular data and evaluating the model\n",
    "\n",
    "First, the model name and the necessary filepaths to save the model are defined in the code block below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model name and paths\n",
    "model_name = \"cnn_baseline\"\n",
    "save_filepath = f\"trained_models/{model_name}.tf\"\n",
    "model_filepath = f\"trained_models/{model_name}.json\"\n",
    "weights_filepath = f\"trained_models/{model_name}_weights.hdf5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The code block directly below can be used if the model is <font color='red'>already trained</font> and if the structure & weights are saved.** Training the model is not required in this case. The code block can be skipped if this is not the case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at trained_models/cnn_baseline.tf",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load weights into the model (ONLY IF MODEL ALREADY EXISTS!!!)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model_cnn \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_filepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Variables for ROC analysis\u001b[39;00m\n\u001b[0;32m      5\u001b[0m train_steps \u001b[38;5;241m=\u001b[39m train_gen\u001b[38;5;241m.\u001b[39mn\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mtrain_gen\u001b[38;5;241m.\u001b[39mbatch_size\n",
      "File \u001b[1;32mc:\\Users\\20212059\\.conda\\envs\\8p361\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\20212059\\.conda\\envs\\8p361\\lib\\site-packages\\keras\\saving\\save.py:226\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[1;32m--> 226\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[0;32m    227\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    228\u001b[0m         )\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m    232\u001b[0m             filepath_str, \u001b[38;5;28mcompile\u001b[39m, options\n\u001b[0;32m    233\u001b[0m         )\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at trained_models/cnn_baseline.tf"
     ]
    }
   ],
   "source": [
    "# Load weights into the model (ONLY IF MODEL ALREADY EXISTS!!!)\n",
    "model_cnn = load_model(save_filepath)\n",
    "\n",
    "# Variables for ROC analysis\n",
    "train_steps = train_gen.n//train_gen.batch_size\n",
    "val_steps = val_gen.n//val_gen.batch_size\n",
    "\n",
    "# Print statement to check if correct model is loaded\n",
    "print(f\"Successfully loaded the following model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model is not trained yet, the following code blocks can be used to build the model and initiate the training phase. The model architectures are defined within the class `Model_architecture`. Organizing the code into classes instead of piling everything up in a single script makes the code more clear to read and understand, and helps reuse functionality that is already implemented. The class is located in `main_util.py`. In the code block below, an instance to the class is made and the structure of the baseline CNN model is loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_architecture\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 96, 96, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 24, 24, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 1, 1, 64)          147520    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 1, 1, 1)           65        \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1)                0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 166,977\n",
      "Trainable params: 166,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Class instance is made\n",
    "model_cnn = Model_architecture()\n",
    "model_cnn.create_cnn(kernel_size=(3,3), pool_size=(4,4), first_filters=32, second_filters=64)\n",
    "model_cnn.compile_cnn(learning_rate=0.01, momentum=0.95)\n",
    "\n",
    "# Prints a summary of the model structure\n",
    "model_cnn.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize model to JSON\n",
    "model_json = model_cnn.to_json()\n",
    "with open(model_filepath, 'w') as json_file:\n",
    "    json_file.write(model_json) \n",
    "\n",
    "# Define the Tensorboard callback\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(os.path.join(\"logs\", model_name))\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "# Train the model\n",
    "train_steps = train_gen.n//train_gen.batch_size\n",
    "val_steps = val_gen.n//val_gen.batch_size\n",
    "\n",
    "history = model_cnn.fit(train_gen, steps_per_epoch=train_steps,\n",
    "                        validation_data=val_gen,\n",
    "                        validation_steps=val_steps,\n",
    "                        epochs=3,\n",
    "                        callbacks=callbacks_list)\n",
    "\n",
    "model_cnn.save(save_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, ROC curve analysis is performed on the baseline CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting labels and predictions on validation set\n",
    "val_true = val_gen.classes\n",
    "val_probs = model_cnn.predict(val_gen, steps=val_steps)\n",
    "\n",
    "# Calculating false positive rate (fpr), true positive rate (tpr) and AUC\n",
    "fpr, tpr, thresholds = roc_curve(val_true, val_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Generate ROC curve\n",
    "roc = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n",
    "roc.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating the autoencoder model\n",
    "\n",
    "First, the model name and the filepath for the saved model are defined. Next, we need to construct new data generators. The training process of the autoencoder is unsupervised so the class mode of the data generators should be set to `input`. With these new generators, the autoencoder can be trained effectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model name and paths\n",
    "model_name = \"autoencoder_Constantijn\"\n",
    "save_filepath = f\"trained_models/{model_name}.tf\"\n",
    "model_filepath = f\"trained_models/{model_name}.json\"\n",
    "weights_filepath = f\"trained_models/{model_name}_weights.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Constructing the data generators for unsupervised learning for autoencoder training\n",
    "path = \"../\"\n",
    "train_gen_ae, val_gen_ae = get_pcam_generators(path, \n",
    "                                               train_batch_size=16, \n",
    "                                               val_batch_size=16, \n",
    "                                               class_mode=\"input\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The code block below can be used if there <font color='red'>already exists a trained version</font> of the autoencoder.** The weights are loaded into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the following model: autoencoder_Constantijn\n"
     ]
    }
   ],
   "source": [
    "# Load weights into the model (ONLY IF MODEL ALREADY EXISTS!!!)\n",
    "model_ae = load_model(save_filepath)\n",
    "\n",
    "# Variables for ROC analysis\n",
    "train_steps = train_gen_ae.n//train_gen_ae.batch_size\n",
    "val_steps = val_gen_ae.n//val_gen_ae.batch_size\n",
    "\n",
    "# Print statement to check if correct model is loaded\n",
    "print(f\"Successfully loaded the following model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training is still required, the following code block can be used to construct the model and initiate the training phase. A new instance of the `Model_architecture` class is made for the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_architecture_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 96, 96, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 48, 48, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 48, 48, 16)        4624      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 24, 24, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 24, 24, 16)        2320      \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2D  (None, 48, 48, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 48, 48, 32)        4640      \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSampling  (None, 96, 96, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 96, 96, 3)         867       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,347\n",
      "Trainable params: 13,347\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_ae = Model_architecture()\n",
    "model_ae.create_autoencoder(kernel_size=(3,3), pool_size=(2,2), first_filters=32, second_filters=16)\n",
    "model_ae.compile_autoencoder(learning_rate=0.001)\n",
    "\n",
    "model_ae.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize model to JSON\n",
    "model_json = model_ae.to_json() \n",
    "with open(model_filepath, 'w') as json_file:\n",
    "    json_file.write(model_json) \n",
    "\n",
    "# Define the Tensorboard callback\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(os.path.join(\"logs\", model_name))\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "# Train the model\n",
    "train_steps = train_gen_ae.n//train_gen_ae.batch_size\n",
    "val_steps = val_gen_ae.n//val_gen_ae.batch_size\n",
    "\n",
    "history = model_ae.fit(train_gen_ae, steps_per_epoch=train_steps,\n",
    "                        validation_data=val_gen_ae,\n",
    "                        validation_steps=val_steps,\n",
    "                        epochs=3,\n",
    "                        callbacks=callbacks_list)\n",
    "\n",
    "model_ae.save(save_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the autoencoder model is visualized. This output  is used as augmented dataset in the upcoming steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a prediction on the validation set\n",
    "img_batch = train_gen_ae[0][1] # [batch][class][image_nr]\n",
    "predict_test = model_ae.predict(img_batch) \n",
    "image_nr = 3\n",
    "\n",
    "fig,ax = plt.subplots(1,2)\n",
    "ax[0].imshow(img_batch[image_nr])\n",
    "ax[0].set_title(\"Original image\")\n",
    "ax[1].imshow(predict_test[image_nr])\n",
    "ax[1].set_title(\"Reconstructed image\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training CNN with augmented data using autoencoder\n",
    "\n",
    "The generators for the augmented data are initialized with the autoencoder model as preprocessing function. This is done with the class `Model_transform`. This class is located in `main_util.py` and is responsible for augmenting the input of the data generators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Constructing the data generators for the augmented dataset  \n",
    "transformation = Model_transform(ae_model=model_ae, augmentation_factor=0.5)\n",
    "train_gen_aug, val_gen_aug = get_pcam_generators(path, \n",
    "                                                 train_batch_size=16,\n",
    "                                                 val_batch_size=16,\n",
    "                                                 class_mode=\"binary\", \n",
    "                                                 prep_function=transformation.model_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is to see if the generator works properly. It plots a few images of a batch to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1, 8)\n",
    "for images, labels in train_gen_aug:\n",
    "    for i in range(8):\n",
    "        ax[i].imshow(images[i])\n",
    "        ax[i].axis(\"off\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model name and filepaths for the saved model are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model name and paths\n",
    "model_name = \"cnn_augmented_Constantijn\"\n",
    "save_filepath = f\"trained_models/{model_name}.tf\"\n",
    "model_filepath = f\"trained_models/{model_name}.json\"\n",
    "weights_filepath = f\"trained_models/{model_name}_weights.hdf5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The code block below can be used if there <font color='red'>already exists a trained version</font> of the CNN with augmented data.** The weights are loaded into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights into the model (only if model structure & weight already exist!)\n",
    "cnn_aug_model = load_model(save_filepath)\n",
    "\n",
    "# Variables for ROC analysis\n",
    "train_steps = train_gen_aug.n//train_gen_aug.batch_size\n",
    "val_steps = val_gen_aug.n//val_gen_aug.batch_size\n",
    "\n",
    "# Print statement to check if correct model is loaded\n",
    "print(f\"Successfully loaded the following model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training is still required, the following code block can be used to construct the model and initiate the training phase. A new instance of the `Model_architecture` class is made for the CNN trained with augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_architecture\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 96, 96, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 24, 24, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 1, 1, 64)          147520    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 1, 1, 1)           65        \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1)                0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 166,977\n",
      "Trainable params: 166,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_cnn_aug = Model_architecture()\n",
    "model_cnn_aug.create_cnn(kernel_size=(3,3), pool_size=(4,4), first_filters=32, second_filters=64)\n",
    "model_cnn_aug.compile_cnn(learning_rate=0.01, momentum=0.95)\n",
    "\n",
    "model_cnn_aug.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - ETA: 0s - loss: 0.5016 - accuracy: 0.7557\n",
      "Epoch 1: val_loss improved from inf to 0.45517, saving model to trained_models\\cnn_augmented_Constantijn_weights.hdf5\n",
      "9000/9000 [==============================] - 7916s 879ms/step - loss: 0.5016 - accuracy: 0.7557 - val_loss: 0.4552 - val_accuracy: 0.7931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trained_models/cnn_augmented_Constantijn.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trained_models/cnn_augmented_Constantijn.tf\\assets\n"
     ]
    }
   ],
   "source": [
    "# Serialize model to JSON\n",
    "model_json = model_cnn_aug.to_json() \n",
    "with open(model_filepath, 'w') as json_file:\n",
    "    json_file.write(model_json) \n",
    "\n",
    "# Define the Tensorboard callback\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(os.path.join(\"logs\", model_name))\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "# Train the model\n",
    "train_steps = train_gen_aug.n//train_gen_aug.batch_size\n",
    "val_steps = val_gen_aug.n//val_gen_aug.batch_size\n",
    "\n",
    "history = model_cnn_aug.fit(train_gen_aug, steps_per_epoch=train_steps,\n",
    "                        validation_data=val_gen_aug,\n",
    "                        validation_steps=val_steps,\n",
    "                        epochs=1, \n",
    "                        callbacks=callbacks_list)\n",
    "\n",
    "model_cnn_aug.save(save_filepath)\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting labels and predictions on validation set\n",
    "val_true = val_gen_aug.classes\n",
    "val_probs = model_cnn_aug.predict(val_gen_aug, steps=val_steps)\n",
    "\n",
    "# Calculating false positive rate (fpr), true positive rate (tpr) and AUC\n",
    "fpr, tpr, thresholds = roc_curve(val_true, val_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Generate ROC curve\n",
    "roc = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n",
    "roc.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
