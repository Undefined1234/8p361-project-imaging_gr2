{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder with Convolutional Neural Network\n",
    "This notebook contains the implementation of an autoencoder for augmentation of the histopathology dataset. The augmented data is used to train CNN networks to see if the accuracy and AUC are higher compared to the baseline CNN model trained on the normal dataset. First, the required libraries are imported and the size of the images in the PCAM dataset is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the functions and classes from main_util.py\n",
    "from main_util import get_pcam_generators\n",
    "from main_util import Model_architecture\n",
    "from main_util import Model_transform\n",
    "\n",
    "# Standard libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "# Modelcheckpoint and tensorboard callbacks\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# ROC curve analysis\n",
    "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
    "\n",
    "# the size of the images in the PCAM dataset\n",
    "IMAGE_SIZE = 96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating data generators\n",
    "\n",
    "The PatchCAMELYON dataset is too big to fit in the working memory of most personal computers. This is why, we need to define some functions that will read the image data batch by batch, so only a single batch of images needs to be stored in memory at one time point. We can use the handy ImageDataGenerator function from the Keras API to do this. Note that the generators are defined within the function `get_pcam_generators` that returns them as output arguments. This function will later be called from the main code body. The function is located in `main_util.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before executing the code block below, do not forget to change the path where the PatchCAMELYON dataset is located (that is, the location of the folder that contains `train+val` that you previously downloaded and unpacked).\n",
    "\n",
    "If everything is correct, the following output will be printed on screen after executing the code block:\n",
    "\n",
    "`Found 144000 images belonging to 2 classes.`\n",
    "\n",
    "`Found 16000 images belonging to 2 classes.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "path = 'C:/Users/20212150/Documents/Year 3/Q3/Project AI mia'\n",
    "train_gen, val_gen = get_pcam_generators(path, \n",
    "                                         train_batch_size=16, \n",
    "                                         val_batch_size=16,\n",
    "                                         class_mode=\"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the baseline CNN model on regular data and evaluating the model\n",
    "\n",
    "First, the model name and the necessary filepaths to save the model are defined in the code block below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model name and paths\n",
    "model_name = \"cnn_baseline\"\n",
    "save_filepath = f\"trained_models/{model_name}.tf\"\n",
    "model_filepath = f\"trained_models/{model_name}.json\"\n",
    "weights_filepath = f\"trained_models/{model_name}_weights.hdf5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The code block directly below can be used if the model is <font color='red'>already trained</font> and if the structure & weights are saved.** Training the model is not required in this case. The code block can be skipped if this is not the case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights into the model (ONLY IF MODEL ALREADY EXISTS!!!)\n",
    "model_cnn = load_model(save_filepath)\n",
    "\n",
    "# Variables for ROC analysis\n",
    "train_steps = train_gen.n//train_gen.batch_size\n",
    "val_steps = val_gen.n//val_gen.batch_size\n",
    "\n",
    "# Print statement to check if correct model is loaded\n",
    "print(f\"Successfully loaded the following model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model is not trained yet, the following code blocks can be used to build the model and initiate the training phase. The model architectures are defined within the class `Model_architecture`. Organizing the code into classes instead of piling everything up in a single script makes the code more clear to read and understand, and helps reuse functionality that is already implemented. The class is located in `main_util.py`. In the code block below, an instance to the class is made and the structure of the baseline CNN model is loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class instance is made\n",
    "model_cnn = Model_architecture()\n",
    "model_cnn.create_cnn(kernel_size=(3,3), pool_size=(4,4), first_filters=32, second_filters=64)\n",
    "model_cnn.compile_cnn(learning_rate=0.001)\n",
    "\n",
    "# Prints a summary of the model structure\n",
    "model_cnn.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize model to JSON\n",
    "model_json = model_cnn.to_json()\n",
    "with open(model_filepath, 'w') as json_file:\n",
    "    json_file.write(model_json) \n",
    "\n",
    "# Define the Tensorboard callback\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(os.path.join(\"logs\", model_name))\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "# Train the model\n",
    "train_steps = train_gen.n//train_gen.batch_size//10\n",
    "val_steps = val_gen.n//val_gen.batch_size//10\n",
    "\n",
    "history = model_cnn.fit(train_gen, steps_per_epoch=train_steps,\n",
    "                        validation_data=val_gen,\n",
    "                        validation_steps=val_steps,\n",
    "                        epochs=30,\n",
    "                        callbacks=callbacks_list)\n",
    "\n",
    "model_cnn.save(save_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss and accuracy curves are made for the training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(20,7))\n",
    "\n",
    "# make plot for accuracy\n",
    "ax[0].plot(history.history['accuracy'])\n",
    "ax[0].plot(history.history['val_accuracy'])\n",
    "ax[0].set_title('model accuracy')\n",
    "ax[0].set_ylabel('accuracy')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "\n",
    "# make plot for loss\n",
    "ax[1].plot(history.history['loss'])\n",
    "ax[1].plot(history.history['val_loss'])\n",
    "ax[1].set_title('model loss')\n",
    "ax[1].set_ylabel('loss')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "basedir = '.../project_map/main_project/metric_images/'\n",
    "filename = basedir + model_name + '.png'\n",
    "plt.savefig(filename)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, ROC curve analysis is performed on the baseline CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting labels and predictions on validation set\n",
    "val_true = val_gen.classes\n",
    "val_probs = model_cnn.predict(val_gen, steps=val_steps*10)\n",
    "\n",
    "# Calculating false positive rate (fpr), true positive rate (tpr) and AUC\n",
    "fpr, tpr, thresholds = roc_curve(val_true, val_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Generate ROC curve\n",
    "roc = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "roc.plot(ax=ax1);\n",
    "filename_auc = basedir + model_name + '_auc.png'\n",
    "plt.savefig(filename_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating the autoencoder model\n",
    "\n",
    "First, the model name and the filepath for the saved model are defined. Next, we need to construct new data generators. The training process of the autoencoder is unsupervised so the class mode of the data generators should be set to `input`. With these new generators, the autoencoder can be trained effectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model name and paths\n",
    "model_name = \"autoencoder\"\n",
    "save_filepath = f\"trained_models/{model_name}.tf\"\n",
    "model_filepath = f\"trained_models/{model_name}.json\"\n",
    "weights_filepath = f\"trained_models/{model_name}_weights.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Constructing the data generators for unsupervised learning for autoencoder training\n",
    "train_gen_ae, val_gen_ae = get_pcam_generators(path, \n",
    "                                               train_batch_size=16, \n",
    "                                               val_batch_size=16, \n",
    "                                               class_mode=\"input\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The code block below can be used if there <font color='red'>already exists a trained version</font> of the autoencoder.** The weights are loaded into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot parse keras metadata at path trained_models/autoencoder.tf\\keras_metadata.pb: Received error: Error parsing message",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load weights into the model (ONLY IF MODEL ALREADY EXISTS!!!)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model_ae \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_filepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Variables for ROC analysis\u001b[39;00m\n\u001b[0;32m      5\u001b[0m train_steps \u001b[38;5;241m=\u001b[39m train_gen_ae\u001b[38;5;241m.\u001b[39mn\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mtrain_gen_ae\u001b[38;5;241m.\u001b[39mbatch_size\n",
      "File \u001b[1;32mc:\\Users\\20212150\\.conda\\envs\\8p361\\lib\\site-packages\\keras\\src\\saving\\saving_api.py:238\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    231\u001b[0m         filepath,\n\u001b[0;32m    232\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    233\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    234\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    235\u001b[0m     )\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\20212150\\.conda\\envs\\8p361\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\20212150\\.conda\\envs\\8p361\\lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:113\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, compile, options)\u001b[0m\n\u001b[0;32m    111\u001b[0m         metadata\u001b[38;5;241m.\u001b[39mParseFromString(file_content)\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m message\u001b[38;5;241m.\u001b[39mDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 113\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[0;32m    114\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot parse keras metadata at path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_to_metadata_pb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    115\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m         )\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSavedModel saved prior to TF 2.5 detected when loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras model. Please ensure that you are saving the model \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamed \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras_metadata.pb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the SavedModel directory.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    124\u001b[0m     )\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot parse keras metadata at path trained_models/autoencoder.tf\\keras_metadata.pb: Received error: Error parsing message"
     ]
    }
   ],
   "source": [
    "# Load weights into the model (ONLY IF MODEL ALREADY EXISTS!!!)\n",
    "model_ae = load_model(save_filepath)\n",
    "\n",
    "# Variables for ROC analysis\n",
    "train_steps = train_gen_ae.n//train_gen_ae.batch_size\n",
    "val_steps = val_gen_ae.n//val_gen_ae.batch_size\n",
    "\n",
    "# Print statement to check if correct model is loaded\n",
    "print(f\"Successfully loaded the following model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training is still required, the following code block can be used to construct the model and initiate the training phase. A new instance of the `Model_architecture` class is made for the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ae = Model_architecture()\n",
    "model_ae.create_autoencoder(kernel_size=(3,3), pool_size=(2,2), first_filters=32, second_filters=16)\n",
    "model_ae.compile_autoencoder(learning_rate=0.001)\n",
    "\n",
    "model_ae.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize model to JSON\n",
    "model_json = model_ae.to_json() \n",
    "with open(model_filepath, 'w') as json_file:\n",
    "    json_file.write(model_json) \n",
    "\n",
    "# Define the Tensorboard callback\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(os.path.join(\"logs\", model_name))\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "# Train the model\n",
    "train_steps = train_gen_ae.n//train_gen_ae.batch_size\n",
    "val_steps = val_gen_ae.n//val_gen_ae.batch_size\n",
    "\n",
    "history = model_ae.fit(train_gen_ae, steps_per_epoch=train_steps,\n",
    "                        validation_data=val_gen_ae,\n",
    "                        validation_steps=val_steps,\n",
    "                        epochs=3,\n",
    "                        callbacks=callbacks_list)\n",
    "\n",
    "model_ae.save(save_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the autoencoder model is visualized. This output  is used as augmented dataset in the upcoming steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a prediction on the validation set\n",
    "img_batch = train_gen_ae[0][1] # [batch][class][image_nr]\n",
    "predict_test = model_ae.predict(img_batch) \n",
    "image_nr = 3\n",
    "\n",
    "fig,ax = plt.subplots(1,2)\n",
    "ax[0].imshow(img_batch[image_nr])\n",
    "ax[0].set_title(\"Original image\")\n",
    "ax[1].imshow(predict_test[image_nr])\n",
    "ax[1].set_title(\"Reconstructed image\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training CNN with augmented data using autoencoder\n",
    "\n",
    "The generators for the augmented data are initialized with the autoencoder model as preprocessing function. This is done with the class `Model_transform`. This class is located in `main_util.py` and is responsible for augmenting the input of the data generators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the data generators for the augmented dataset  \n",
    "transformation = Model_transform(ae_model=model_ae, augmentation_factor=0.5)\n",
    "train_gen_aug, val_gen_aug = get_pcam_generators(path, \n",
    "                                                 train_batch_size=16,\n",
    "                                                 val_batch_size=16,\n",
    "                                                 class_mode=\"binary\", \n",
    "                                                 prep_function=transformation.model_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is to see if the generator works properly. It plots a few images of a batch to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1, 8)\n",
    "for images, labels in train_gen_aug:\n",
    "    for i in range(8):\n",
    "        ax[i].imshow(images[i])\n",
    "        ax[i].axis(\"off\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model name and filepaths for the saved model are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model name and paths\n",
    "model_name = \"cnn_augmented\"\n",
    "save_filepath = f\"trained_models/{model_name}.tf\"\n",
    "model_filepath = f\"trained_models/{model_name}.json\"\n",
    "weights_filepath = f\"trained_models/{model_name}_weights.hdf5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The code block below can be used if there <font color='red'>already exists a trained version</font> of the CNN with augmented data.** The weights are loaded into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights into the model (only if model structure & weight already exist!)\n",
    "cnn_aug_model = load_model(save_filepath)\n",
    "\n",
    "# Variables for ROC analysis\n",
    "train_steps = train_gen_aug.n//train_gen_aug.batch_size\n",
    "val_steps = val_gen_aug.n//val_gen_aug.batch_size\n",
    "\n",
    "# Print statement to check if correct model is loaded\n",
    "print(f\"Successfully loaded the following model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training is still required, the following code block can be used to construct the model and initiate the training phase. A new instance of the `Model_architecture` class is made for the CNN trained with augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_aug = Model_architecture()\n",
    "model_cnn_aug.create_cnn(kernel_size=(3,3), pool_size=(4,4), first_filters=32, second_filters=64)\n",
    "model_cnn_aug.compile_cnn(learning_rate=0.001)\n",
    "\n",
    "model_cnn_aug.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize model to JSON\n",
    "model_json = model_cnn_aug.to_json() \n",
    "with open(model_filepath, 'w') as json_file:\n",
    "    json_file.write(model_json) \n",
    "\n",
    "# Define the Tensorboard callback\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(os.path.join(\"logs\", model_name))\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "# Train the model\n",
    "train_steps = train_gen_aug.n//train_gen_aug.batch_size//10\n",
    "val_steps = val_gen_aug.n//val_gen_aug.batch_size//10\n",
    "\n",
    "history = model_cnn_aug.fit(train_gen_aug, steps_per_epoch=train_steps,\n",
    "                        validation_data=val_gen_aug,\n",
    "                        validation_steps=val_steps,\n",
    "                        epochs=30, \n",
    "                        callbacks=callbacks_list)\n",
    "\n",
    "model_cnn_aug.save(save_filepath)\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss and accuracy curves are made for the training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(20,7))\n",
    "\n",
    "# make plot for accuracy\n",
    "ax[0].plot(history.history['accuracy'])\n",
    "ax[0].plot(history.history['val_accuracy'])\n",
    "ax[0].set_title('model accuracy')\n",
    "ax[0].set_ylabel('accuracy')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "\n",
    "# make plot for loss\n",
    "ax[1].plot(history.history['loss'])\n",
    "ax[1].plot(history.history['val_loss'])\n",
    "ax[1].set_title('model loss')\n",
    "ax[1].set_ylabel('loss')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "basedir = '.../project_map/main_project/metric_images/'\n",
    "filename = basedir + model_name + '.png'\n",
    "plt.savefig(filename)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting labels and predictions on validation set\n",
    "val_true = val_gen_aug.classes\n",
    "val_probs = model_cnn_aug.predict(val_gen_aug, steps=val_steps*10)\n",
    "\n",
    "# Calculating false positive rate (fpr), true positive rate (tpr) and AUC\n",
    "fpr, tpr, thresholds = roc_curve(val_true, val_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Generate ROC curve\n",
    "roc = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n",
    "roc.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
