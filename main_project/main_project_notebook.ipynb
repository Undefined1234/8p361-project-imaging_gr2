{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder with Convolutional Neural Network\n",
    "This notebook contains the implementation of an autoencoder for augmentation of the histopathology dataset. The augmented data is used to train CNN networks to see if the accuracy and AUC are higher compared to the baseline CNN model trained on the normal dataset. First, the required libraries are imported and the size of the images in the PCAM dataset is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the functions and classes from main_util.py\n",
    "from main_util import get_pcam_generators\n",
    "from main_util import Model_architecture\n",
    "from main_util import Model_transform\n",
    "\n",
    "# Standard libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "# Modelcheckpoint and tensorboard callbacks\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# ROC curve analysis\n",
    "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
    "\n",
    "# the size of the images in the PCAM dataset\n",
    "IMAGE_SIZE = 96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating data generators\n",
    "\n",
    "The PatchCAMELYON dataset is too big to fit in the working memory of most personal computers. This is why, we need to define some functions that will read the image data batch by batch, so only a single batch of images needs to be stored in memory at one time point. We can use the handy ImageDataGenerator function from the Keras API to do this. Note that the generators are defined within the function `get_pcam_generators` that returns them as output arguments. This function will later be called from the main code body. The function is located in `main_util.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before executing the code block below, do not forget to change the path where the PatchCAMELYON dataset is located (that is, the location of the folder that contains `train+val` that you previously downloaded and unpacked).\n",
    "\n",
    "If everything is correct, the following output will be printed on screen after executing the code block:\n",
    "\n",
    "`Found 144000 images belonging to 2 classes.`\n",
    "\n",
    "`Found 16000 images belonging to 2 classes.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "path = \"../\"\n",
    "train_gen, val_gen = get_pcam_generators(path, \n",
    "                                         train_batch_size=16, \n",
    "                                         val_batch_size=16,\n",
    "                                         class_mode=\"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the baseline CNN model on regular data and evaluating the model\n",
    "\n",
    "First, the model name and the necessary filepaths to save the model are defined in the code block below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model name and paths\n",
    "model_name = \"cnn_baseline\"\n",
    "save_filepath = f\"trained_models/{model_name}.tf\"\n",
    "model_filepath = f\"trained_models/{model_name}.json\"\n",
    "weights_filepath = f\"trained_models/{model_name}_weights.hdf5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The code block directly below can be used if the model is <font color='red'>already trained</font> and if the structure & weights are saved.** Training the model is not required in this case. The code block can be skipped if this is not the case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the following model: autoencoder\n"
     ]
    }
   ],
   "source": [
    "# Load weights into the model (ONLY IF MODEL ALREADY EXISTS!!!)\n",
    "model_cnn = load_model(save_filepath)\n",
    "\n",
    "# Variables for ROC analysis\n",
    "train_steps = train_gen.n//train_gen.batch_size\n",
    "val_steps = val_gen.n//val_gen.batch_size\n",
    "\n",
    "# Print statement to check if correct model is loaded\n",
    "print(f\"Successfully loaded the following model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model is not trained yet, the following code blocks can be used to build the model and initiate the training phase. The model architectures are defined within the class `Model_architecture`. Organizing the code into classes instead of piling everything up in a single script makes the code more clear to read and understand, and helps reuse functionality that is already implemented. The class is located in `main_util.py`. In the code block below, an instance to the class is made and the structure of the baseline CNN model is loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_architecture_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_10 (Conv2D)          (None, 96, 96, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 24, 24, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 1, 1, 64)          147520    \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 1, 1, 1)           65        \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1)                0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 166,977\n",
      "Trainable params: 166,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Class instance is made\n",
    "model_cnn = Model_architecture()\n",
    "model_cnn.create_cnn(kernel_size=(3,3), pool_size=(4,4), first_filters=32, second_filters=64)\n",
    "model_cnn.compile_cnn(learning_rate=0.001)\n",
    "\n",
    "# Prints a summary of the model structure\n",
    "model_cnn.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize model to JSON\n",
    "model_json = model_cnn.to_json()\n",
    "with open(model_filepath, 'w') as json_file:\n",
    "    json_file.write(model_json) \n",
    "\n",
    "# Define the Tensorboard callback\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(os.path.join(\"logs\", model_name))\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "# Train the model\n",
    "train_steps = train_gen.n//train_gen.batch_size//10\n",
    "val_steps = val_gen.n//val_gen.batch_size//10\n",
    "\n",
    "history = model_cnn.fit(train_gen, steps_per_epoch=train_steps,\n",
    "                        validation_data=val_gen,\n",
    "                        validation_steps=val_steps,\n",
    "                        epochs=30,\n",
    "                        callbacks=callbacks_list)\n",
    "\n",
    "model_cnn.save(save_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss and accuracy curves are made for the training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(20,7))\n",
    "\n",
    "# make plot for accuracy\n",
    "ax[0].plot(history.history['accuracy'])\n",
    "ax[0].plot(history.history['val_accuracy'])\n",
    "ax[0].set_title('model accuracy')\n",
    "ax[0].set_ylabel('accuracy')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "\n",
    "# make plot for loss\n",
    "ax[1].plot(history.history['loss'])\n",
    "ax[1].plot(history.history['val_loss'])\n",
    "ax[1].set_title('model loss')\n",
    "ax[1].set_ylabel('loss')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "basedir = '.../project_map/main_project/metric_images/'\n",
    "filename = basedir + model_name + '.png'\n",
    "plt.savefig(filename)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, ROC curve analysis is performed on the baseline CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting labels and predictions on validation set\n",
    "val_true = val_gen.classes\n",
    "val_probs = model_cnn.predict(val_gen, steps=val_steps*10)\n",
    "\n",
    "# Calculating false positive rate (fpr), true positive rate (tpr) and AUC\n",
    "fpr, tpr, thresholds = roc_curve(val_true, val_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Generate ROC curve\n",
    "roc = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "roc.plot(ax=ax1);\n",
    "filename_auc = basedir + model_name + '_auc.png'\n",
    "plt.savefig(filename_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating the autoencoder model\n",
    "\n",
    "First, the model name and the filepath for the saved model are defined. Next, we need to construct new data generators. The training process of the autoencoder is unsupervised so the class mode of the data generators should be set to `input`. With these new generators, the autoencoder can be trained effectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model name and paths\n",
    "model_name = \"autoencoder.tf\"\n",
    "save_filepath = f\"trained_models/{model_name}.tf\"\n",
    "model_filepath = f\"trained_models/{model_name}.json\"\n",
    "weights_filepath = f\"trained_models/{model_name}_weights.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Constructing the data generators for unsupervised learning for autoencoder training\n",
    "train_gen_ae, val_gen_ae = get_pcam_generators(path, \n",
    "                                               train_batch_size=16, \n",
    "                                               val_batch_size=16, \n",
    "                                               class_mode=\"input\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The code block below can be used if there <font color='red'>already exists a trained version</font> of the autoencoder.** The weights are loaded into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the following model: autoencoder\n"
     ]
    }
   ],
   "source": [
    "# Load weights into the model (ONLY IF MODEL ALREADY EXISTS!!!)\n",
    "model_ae = load_model(save_filepath)\n",
    "\n",
    "# Variables for ROC analysis\n",
    "train_steps = train_gen_ae.n//train_gen_ae.batch_size\n",
    "val_steps = val_gen_ae.n//val_gen_ae.batch_size\n",
    "\n",
    "# Print statement to check if correct model is loaded\n",
    "print(f\"Successfully loaded the following model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training is still required, the following code block can be used to construct the model and initiate the training phase. A new instance of the `Model_architecture` class is made for the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_architecture_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_5 (Conv2D)           (None, 96, 96, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 48, 48, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 48, 48, 16)        4624      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 24, 24, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 24, 24, 16)        2320      \n",
      "                                                                 \n",
      " up_sampling2d_2 (UpSampling  (None, 48, 48, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 48, 48, 32)        4640      \n",
      "                                                                 \n",
      " up_sampling2d_3 (UpSampling  (None, 96, 96, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 96, 96, 3)         867       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,347\n",
      "Trainable params: 13,347\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_ae = Model_architecture()\n",
    "model_ae.create_autoencoder(kernel_size=(3,3), pool_size=(2,2), first_filters=32, second_filters=16)\n",
    "model_ae.compile_autoencoder(learning_rate=0.001)\n",
    "\n",
    "model_ae.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize model to JSON\n",
    "model_json = model_ae.to_json() \n",
    "with open(model_filepath, 'w') as json_file:\n",
    "    json_file.write(model_json) \n",
    "\n",
    "# Define the Tensorboard callback\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(os.path.join(\"logs\", model_name))\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "# Train the model\n",
    "train_steps = train_gen_ae.n//train_gen_ae.batch_size\n",
    "val_steps = val_gen_ae.n//val_gen_ae.batch_size\n",
    "\n",
    "history = model_ae.fit(train_gen_ae, steps_per_epoch=train_steps,\n",
    "                        validation_data=val_gen_ae,\n",
    "                        validation_steps=val_steps,\n",
    "                        epochs=3,\n",
    "                        callbacks=callbacks_list)\n",
    "\n",
    "model_ae.save(save_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the autoencoder model is visualized. This output  is used as augmented dataset in the upcoming steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_UserObject' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Produce a prediction on the validation set\u001b[39;00m\n\u001b[0;32m      2\u001b[0m img_batch \u001b[38;5;241m=\u001b[39m train_gen_ae[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# [batch][class][image_nr]\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m predict_test \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_ae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(img_batch) \n\u001b[0;32m      4\u001b[0m image_nr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m      6\u001b[0m fig,ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_UserObject' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "# Produce a prediction on the validation set\n",
    "img_batch = train_gen_ae[0][1] # [batch][class][image_nr]\n",
    "predict_test = model_ae.predict(img_batch) \n",
    "image_nr = 3\n",
    "\n",
    "fig,ax = plt.subplots(1,2)\n",
    "ax[0].imshow(img_batch[image_nr])\n",
    "ax[0].set_title(\"Original image\")\n",
    "ax[1].imshow(predict_test[image_nr])\n",
    "ax[1].set_title(\"Reconstructed image\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training CNN with augmented data using autoencoder\n",
    "\n",
    "The generators for the augmented data are initialized with the autoencoder model as preprocessing function. This is done with the class `Model_transform`. This class is located in `main_util.py` and is responsible for augmenting the input of the data generators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Constructing the data generators for the augmented dataset  \n",
    "transformation = Model_transform(ae_model=model_ae, augmentation_factor=0.25)\n",
    "train_gen_aug, val_gen_aug = get_pcam_generators(path, \n",
    "                                                 train_batch_size=16,\n",
    "                                                 val_batch_size=16,\n",
    "                                                 class_mode=\"binary\", \n",
    "                                                 prep_function=transformation.model_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is to see if the generator works properly. It plots a few images of a batch to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1, 8)\n",
    "for images, labels in train_gen_aug:\n",
    "    for i in range(8):\n",
    "        ax[i].imshow(images[i])\n",
    "        ax[i].axis(\"off\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model name and filepaths for the saved model are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model name and paths\n",
    "model_name = \"cnn_augmented_Constantijn_augfactor25\"\n",
    "save_filepath = f\"trained_models/{model_name}.tf\"\n",
    "model_filepath = f\"trained_models/{model_name}.json\"\n",
    "weights_filepath = f\"trained_models/{model_name}_weights.hdf5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The code block below can be used if there <font color='red'>already exists a trained version</font> of the CNN with augmented data.** The weights are loaded into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights into the model (only if model structure & weight already exist!)\n",
    "cnn_aug_model = load_model(save_filepath)\n",
    "\n",
    "# Variables for ROC analysis\n",
    "train_steps = train_gen_aug.n//train_gen_aug.batch_size\n",
    "val_steps = val_gen_aug.n//val_gen_aug.batch_size\n",
    "\n",
    "# Print statement to check if correct model is loaded\n",
    "print(f\"Successfully loaded the following model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training is still required, the following code block can be used to construct the model and initiate the training phase. A new instance of the `Model_architecture` class is made for the CNN trained with augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_aug = Model_architecture()\n",
    "model_cnn_aug.create_cnn(kernel_size=(3,3), pool_size=(4,4), first_filters=32, second_filters=64)\n",
    "model_cnn_aug.compile_cnn(learning_rate=0.001)\n",
    "\n",
    "model_cnn_aug.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize model to JSON\n",
    "model_json = model_cnn_aug.to_json() \n",
    "with open(model_filepath, 'w') as json_file:\n",
    "    json_file.write(model_json) \n",
    "\n",
    "# Define the Tensorboard callback\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(os.path.join(\"logs\", model_name))\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "# Train the model\n",
    "train_steps = train_gen_aug.n//train_gen_aug.batch_size//10\n",
    "val_steps = val_gen_aug.n//val_gen_aug.batch_size//10\n",
    "\n",
    "history = model_cnn_aug.fit(train_gen_aug, steps_per_epoch=train_steps,\n",
    "                        validation_data=val_gen_aug,\n",
    "                        validation_steps=val_steps,\n",
    "                        epochs=30, \n",
    "                        callbacks=callbacks_list)\n",
    "\n",
    "model_cnn_aug.save(save_filepath)\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss and accuracy curves are made for the training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(20,7))\n",
    "\n",
    "# make plot for accuracy\n",
    "ax[0].plot(history.history['accuracy'])\n",
    "ax[0].plot(history.history['val_accuracy'])\n",
    "ax[0].set_title('model accuracy')\n",
    "ax[0].set_ylabel('accuracy')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "\n",
    "# make plot for loss\n",
    "ax[1].plot(history.history['loss'])\n",
    "ax[1].plot(history.history['val_loss'])\n",
    "ax[1].set_title('model loss')\n",
    "ax[1].set_ylabel('loss')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "basedir = '.../project_map/main_project/metric_images/'\n",
    "filename = basedir + model_name + '.png'\n",
    "plt.savefig(filename)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting labels and predictions on validation set\n",
    "val_true = val_gen_aug.classes\n",
    "val_probs = model_cnn_aug.predict(val_gen_aug, steps=val_steps*10)\n",
    "\n",
    "# Calculating false positive rate (fpr), true positive rate (tpr) and AUC\n",
    "fpr, tpr, thresholds = roc_curve(val_true, val_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Generate ROC curve\n",
    "roc = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n",
    "roc.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
