{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder for binary classification\n",
    "\n",
    "In this notebook, I will experiment with the implementation of an autoencoder for binary classification of the histopathology dataset. First, the required libraries are imported and the size of the images in the PCAM dataset is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, UpSampling2D\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "# ROC analysis\n",
    "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
    "\n",
    "\n",
    "# the size of the images in the PCAM dataset\n",
    "IMAGE_SIZE = 96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating data generators\n",
    "\n",
    "The PatchCAMELYON dataset is too big to fit in the working memory of most personal computers. This is why, we need to define some functions that will read the image data batch by batch, so only a single batch of images needs to be stored in memory at one time point. We can use the handy ImageDataGenerator function from the Keras API to do this. Note that the generators are defined within a function that returns them as output arguments. This function will later be called from the main code body. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pcam_generators(base_dir, train_batch_size=32, val_batch_size=32):\n",
    "\n",
    "     # dataset parameters\n",
    "     TRAIN_PATH = os.path.join(base_dir, 'train+val', 'train')\n",
    "     VALID_PATH = os.path.join(base_dir, 'train+val', 'valid')\n",
    "\n",
    "     RESCALING_FACTOR = 1./255\n",
    "     \n",
    "     # instantiate data generators\n",
    "     datagen = ImageDataGenerator(rescale=RESCALING_FACTOR)\n",
    "\n",
    "     train_gen = datagen.flow_from_directory(TRAIN_PATH,\n",
    "                                             target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                             batch_size=train_batch_size,\n",
    "                                             class_mode='input')\n",
    "\n",
    "     val_gen = datagen.flow_from_directory(VALID_PATH,\n",
    "                                             target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                             batch_size=val_batch_size,\n",
    "                                             class_mode='input',\n",
    "                                             shuffle=False)\n",
    "     \n",
    "     return train_gen, val_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an autoencoder model\n",
    "\n",
    "The autoencoder neural network model is also defined within a function. Organizing the code into functions instead of piling everything up in a single script makes the code more clear to read and understand, and helps reuse functionality that is already implemented. For example, we can use the `get_pcam_generators()` function to create data generators with different batch sizes just by calling the function with a different set of parameters. Or, we can use the `get_model()` function to generate networks with different number of feature maps (see below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_autoencoder(kernel_size=(3,3), pool_size=(2,2), first_filters=32, second_filters=16):\n",
    "\n",
    "\n",
    "     # build the enoder: standard latent representation of (12,12,16)\n",
    "     model = Sequential()\n",
    "\n",
    "     # encoder: standard latent representation of (12,12,16)\n",
    "     model.add(Conv2D(first_filters, kernel_size, activation='relu', padding='same', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)))\n",
    "     model.add(MaxPool2D(pool_size))\n",
    "     model.add(Conv2D(second_filters, kernel_size, activation='relu', padding='same'))\n",
    "     model.add(MaxPool2D(pool_size))\n",
    "\n",
    "     # decoder: upsampling to reconstruct input\n",
    "     model.add(Conv2D(second_filters, kernel_size, activation='relu', padding='same'))\n",
    "     model.add(UpSampling2D(pool_size))\n",
    "     model.add(Conv2D(first_filters, kernel_size, activation='relu', padding='same'))\n",
    "     model.add(UpSampling2D(pool_size))\n",
    "\n",
    "     # reconstructing image: (96,96,1)\n",
    "     model.add(Conv2D(1, kernel_size, activation='sigmoid', padding='same'))\n",
    "    \n",
    "     # compile the model\n",
    "     model.compile(Adam(learning_rate=0.001), loss = 'binary_crossentropy')\n",
    "\n",
    "     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the two functions that define the model and the data generators can be called from the main code body. Before executing the code block below, do not forget to change the path where the PatchCAMELYON dataset is located (that is, the location of the folder that contains `train+val` that you previously downloaded and unpacked).\n",
    "\n",
    "If everything is correct, the following output will be printed on screen after executing the code block:\n",
    "\n",
    "`Found 144000 images belonging to 2 classes.`\n",
    "\n",
    "`Found 16000 images belonging to 2 classes.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# get the model\n",
    "autoencoder = get_autoencoder()\n",
    "\n",
    "# get the data generators\n",
    "train_gen, val_gen = get_pcam_generators('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know the shapes of the outputs of all layers in the network (the dimensionality of the feature maps), you can print them in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 96, 96, 32)\n",
      "(None, 48, 48, 32)\n",
      "(None, 48, 48, 16)\n",
      "(None, 24, 24, 16)\n",
      "(None, 24, 24, 16)\n",
      "(None, 48, 48, 16)\n",
      "(None, 48, 48, 32)\n",
      "(None, 96, 96, 32)\n",
      "(None, 96, 96, 1)\n"
     ]
    }
   ],
   "source": [
    "for layer in autoencoder.layers:\n",
    "    print(layer.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating the model\n",
    "\n",
    "Finally, the model can be trained using data generated by the data generators and then evaluated. This is done in a similar way to the previous assignment. Furthermore, in addition to the Tensorflow callback, an additional callback that saves the \"best\" version of the trained model to a file is added, and the model structure is saved to a json file. This enables loading the model and corresponding weights at a later time point (e.g. when we want to evaluate the model on a test set). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 428/4500 [=>............................] - ETA: 29:17 - loss: 0.5727"
     ]
    }
   ],
   "source": [
    "# save the model and weights\n",
    "model_name = 'first_autoencoder'\n",
    "model_filepath = model_name + '.json'\n",
    "weights_filepath = model_name + '_weights.hdf5'\n",
    "\n",
    "model_json = autoencoder.to_json() # serialize model to JSON\n",
    "with open(model_filepath, 'w') as json_file:\n",
    "    json_file.write(model_json) \n",
    "\n",
    "\n",
    "# define the model checkpoint and Tensorboard callbacks\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(os.path.join('logs', model_name))\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "\n",
    "# train the model\n",
    "train_steps = train_gen.n//train_gen.batch_size\n",
    "val_steps = val_gen.n//val_gen.batch_size\n",
    "\n",
    "history = autoencoder.fit(train_gen, steps_per_epoch=train_steps, \n",
    "                          validation_data=val_gen,\n",
    "                          validation_steps=val_steps,\n",
    "                          epochs=3,\n",
    "                          callbacks=callbacks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
