{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder with Convolutional Neural Network\n",
    "This notebook contains the implementation of an autoencoder for augmentation of the histopathology dataset. The augmented data is used to train CNN networks to see if the accuracy and AUC are higher compared to the baseline CNN model trained on the normal dataset. First, the required libraries are imported and the size of the images in the PCAM dataset is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the functions and classes from main_util.py\n",
    "from main_util import get_pcam_generators\n",
    "from main_util import Model_architecture\n",
    "from main_util import Model_transform\n",
    "\n",
    "# Standard libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Modelcheckpoint and tensorboard callbacks\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "# ROC curve analysis\n",
    "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
    "\n",
    "# the size of the images in the PCAM dataset\n",
    "IMAGE_SIZE = 96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating data generators\n",
    "\n",
    "The PatchCAMELYON dataset is too big to fit in the working memory of most personal computers. This is why, we need to define some functions that will read the image data batch by batch, so only a single batch of images needs to be stored in memory at one time point. We can use the handy ImageDataGenerator function from the Keras API to do this. Note that the generators are defined within the function `get_pcam_generators` that returns them as output arguments. This function will later be called from the main code body. The function is located in `main_util.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before executing the code block below, do not forget to change the path where the PatchCAMELYON dataset is located (that is, the location of the folder that contains `train+val` that you previously downloaded and unpacked).\n",
    "\n",
    "If everything is correct, the following output will be printed on screen after executing the code block:\n",
    "\n",
    "`Found 144000 images belonging to 2 classes.`\n",
    "\n",
    "`Found 16000 images belonging to 2 classes.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data\"\n",
    "train_gen, val_gen = get_pcam_generators(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building model architectures\n",
    "\n",
    "The model architectures are defined within the class `Model_architecture`. Organizing the code into classes instead of piling everything up in a single script makes the code more clear to read and understand, and helps reuse functionality that is already implemented. The class is also located in `main_util.py`. In the code block below, an instance to the class is made and the structure of the baseline CNN model is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class instance is made\n",
    "model_cnn = Model_architecture()\n",
    "model_cnn.create_cnn(kernel_size=(3,3), pool_size=(4,4), first_filters=32, second_filters=64)\n",
    "model_cnn.compile_cnn()\n",
    "model_cnn._name = 'cnn'\n",
    "\n",
    "# Prints a summary of the model structure\n",
    "model_cnn.summary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the baseline CNN model on regular data and evaluating the model\n",
    "\n",
    "After loading the CNN model structure, the training phase will be initiated in the code block below. This is followed by a ROC curve analysis of the trained CNN model. The training is done with the regular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and weights\n",
    "model_name = 'cnn'\n",
    "model_filepath = model_name + '.json'\n",
    "weights_filepath = model_name + '_weights.hdf5'\n",
    "\n",
    "model_json = model_cnn.to_json() # serialize model to JSON\n",
    "with open(model_filepath, 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Define the model checkpoint and Tensorboard callbacks\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(os.path.join('logs', model_name))\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "# Train the model\n",
    "train_steps = train_gen.n//train_gen.batch_size\n",
    "val_steps = val_gen.n//val_gen.batch_size\n",
    "\n",
    "history = model_cnn.fit(train_gen, steps_per_epoch=train_steps,\n",
    "                        validation_data=val_gen,\n",
    "                        validation_steps=val_steps,\n",
    "                        epochs=1,\n",
    "                        callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting labels and predictions on validation set\n",
    "val_true = val_gen.classes\n",
    "val_probs = model_cnn.predict(val_gen, steps=val_steps)\n",
    "\n",
    "# Calculating false positive rate (fpr), true positive rate (tpr) and AUC\n",
    "fpr, tpr, thresholds = roc_curve(val_true, val_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Generate ROC curve\n",
    "roc = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n",
    "roc.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating the autoencoder model\n",
    "\n",
    "First, we need to construct new data generators. The training process of the autoencoder is unsupervised so the class mode of the data generators should be set to `input`. With these new generators, the autoencoder can be trained effectively. A new instance of the `Model_architecture` class is created for the autoencoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the data generators for unsupervised learning for autoencoder training\n",
    "train_gen_ae, val_gen_ae = get_pcam_generators(path, train_batch_size=16, val_batch_size=16, class_mode='input') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ae = Model_architecture()\n",
    "model_ae.create_autoencoder(kernel_size=(3,3), pool_size=(2,2), first_filters=32, second_filters=16)\n",
    "model_ae.compile_autoencoder()\n",
    "model_ae._name = 'Autoencoder'\n",
    "\n",
    "model_ae.summary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the training phase of the autoencoder can be initiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and weights\n",
    "model_name = 'autoencoder'\n",
    "model_filepath = model_name + '.json'\n",
    "weights_filepath = model_name + '_weights.hdf5'\n",
    "\n",
    "model_json = model_ae.to_json() # serialize model to JSON\n",
    "with open(model_filepath, 'w') as json_file:\n",
    "    json_file.write(model_json) \n",
    "\n",
    "# Define the model checkpoint and Tensorboard callbacks\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(os.path.join('logs', model_name))\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "# Train the model\n",
    "train_steps_ae = train_gen_ae.n//train_gen_ae.batch_size\n",
    "val_steps_ae = val_gen_ae.n//val_gen_ae.batch_size\n",
    "\n",
    "history = model_ae.fit(train_gen_ae, steps_per_epoch=train_steps_ae, \n",
    "                       validation_data=val_gen_ae,\n",
    "                       validation_steps=val_steps_ae,\n",
    "                       epochs=3,\n",
    "                       callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the output of the trained autoencoder model. The output of the autoencoder is used as augmented dataset in the upcoming steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a prediction on the validation set\n",
    "img_batch = train_gen_ae[0][1] # [batch][class][image_nr]\n",
    "predict_test = model_ae.predict(img_batch) \n",
    "image_nr = 3\n",
    "\n",
    "fig,ax = plt.subplots(1,2)\n",
    "ax[0].imshow(img_batch[image_nr])\n",
    "ax[0].set_title('Original image')\n",
    "ax[1].imshow(predict_test[image_nr])\n",
    "ax[1].set_title('Reconstructed image');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training CNN with augmented data using autoencoder\n",
    "\n",
    "The generators for the augmented data are initialized with the autoencoder model as preprocessing function. This is done with the class `Model_transform`. This class is located in `main_util.py` and is responsible for augmenting the input of the data generators. The `Model_architecture` class is used again to create a new instance for this model. This model is trained on the augmented dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the data generators for the augmented dataset  \n",
    "train_gen_aug, val_gen_aug = get_pcam_generators(path,\n",
    "                                                 class_mode='binary', \n",
    "                                                 prep_function=Model_transform(model_ae).model_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is to see if the generator works properly. It plots a few images of a batch to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1, 8)\n",
    "for images, labels in train_gen_aug:\n",
    "    for i in range(8):\n",
    "        ax[i].imshow(images[i])\n",
    "        ax[i].axis('off')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_aug = Model_architecture()\n",
    "model_cnn_aug.create_cnn(kernel_size=(3,3), pool_size=(4,4), first_filters=32, second_filters=64)\n",
    "model_cnn_aug.compile_cnn()\n",
    "model_cnn_aug._name = 'cnn_aug'\n",
    "\n",
    "model_cnn_aug.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training phase on the augmented data can now be initiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and weights\n",
    "model_name = 'cnn_aug'\n",
    "model_filepath = model_name + '.json'\n",
    "weights_filepath = model_name + '_weights.hdf5'\n",
    "\n",
    "model_json = model_cnn_aug.to_json() # serialize model to JSON\n",
    "with open(model_filepath, 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Define the model checkpoint and Tensorboard callbacks\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(os.path.join('logs', model_name))\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "# Train the model\n",
    "train_steps_cnn_aug = train_gen_aug.n//train_gen_aug.batch_size\n",
    "val_steps_cnn_aug = val_gen_aug.n//val_gen_aug.batch_size\n",
    "\n",
    "history = model_cnn_aug.fit(train_gen_aug, steps_per_epoch=train_steps_cnn_aug,\n",
    "                            validation_data=val_gen_aug,\n",
    "                            validation_steps=val_steps_cnn_aug,\n",
    "                            epochs=1,\n",
    "                            callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting labels and predictions on validation set\n",
    "val_true = val_gen_aug.classes\n",
    "val_probs = model_cnn_aug.predict(val_gen_aug, steps=val_steps_cnn_aug)\n",
    "\n",
    "# Calculating false positive rate (fpr), true positive rate (tpr) and AUC\n",
    "fpr, tpr, thresholds = roc_curve(val_true, val_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Generate ROC curve\n",
    "roc = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n",
    "roc.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
