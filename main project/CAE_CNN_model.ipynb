{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder with Convolutional Neural Network\n",
    "\n",
    "In this notebook, I will experiment with the implementation of an autoencoder for augmentation of the histopathology dataset. The augmented data is used to train a CNN network to see if it performs better than a baseline CNN model trained on the normal dataset. First, the required libraries are imported and the size of the images in the PCAM dataset is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, GlobalAveragePooling2D, Input, MaxPool2D, UpSampling2D\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
    "\n",
    "# the size of the images in the PCAM dataset\n",
    "IMAGE_SIZE = 96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating data generators\n",
    "\n",
    "The PatchCAMELYON dataset is too big to fit in the working memory of most personal computers. This is why, we need to define some functions that will read the image data batch by batch, so only a single batch of images needs to be stored in memory at one time point. We can use the handy ImageDataGenerator function from the Keras API to do this. Note that the generators are defined within the function `get_pcam_generators` that returns them as output arguments. This function will later be called from the main code body. The class `model_transform` is used to set the correct preprocessing function for the data generators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pcam_generators(base_dir, train_batch_size=32, val_batch_size=32, class_mode='binary', prep_function=None):\n",
    "     # dataset parameters\n",
    "     TRAIN_PATH = os.path.join(base_dir, 'train+val', 'train')\n",
    "     VALID_PATH = os.path.join(base_dir, 'train+val', 'valid')\n",
    "\n",
    "     RESCALING_FACTOR = 1./255\n",
    "\n",
    "     # instantiate data generators\n",
    "     if prep_function is None:\n",
    "          datagen = ImageDataGenerator(rescale=RESCALING_FACTOR)\n",
    "     else:\n",
    "          datagen = ImageDataGenerator(rescale=RESCALING_FACTOR, preprocessing_function=prep_function)\n",
    "\n",
    "     train_gen = datagen.flow_from_directory(TRAIN_PATH,\n",
    "                                             target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                             batch_size=train_batch_size,\n",
    "                                             class_mode=class_mode)\n",
    "\n",
    "\n",
    "     val_gen = datagen.flow_from_directory(VALID_PATH,\n",
    "                                           target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                           batch_size=val_batch_size,\n",
    "                                           class_mode=class_mode)\n",
    "     \n",
    "     return train_gen, val_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the function `get_pcam_generators` that defines the data generators can be called from the main code body. Before executing the code block below, do not forget to change the path where the PatchCAMELYON dataset is located (that is, the location of the folder that contains `train+val` that you previously downloaded and unpacked).\n",
    "\n",
    "If everything is correct, the following output will be printed on screen after executing the code block:\n",
    "\n",
    "`Found 144000 images belonging to 2 classes.`\n",
    "\n",
    "`Found 16000 images belonging to 2 classes.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen, val_gen = get_pcam_generators('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building model architectures\n",
    "\n",
    "The model architectures are defined within a class. Organizing the code into classes instead of piling everything up in a single script makes the code more clear to read and understand, and helps reuse functionality that is already implemented. For example, we can use the `get_pcam_generators` function to create data generators with different batch sizes just by calling the function with a different set of parameters. Or, we can use the `model_architecture` class to generate networks with different number of feature maps (see below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class structure from Constantijn & Nino implemented for AE model from Mart\n",
    "class model_architecture(Sequential):\n",
    "    def __init__(self, kernel_size, pool_size, first_filters, second_filters):\n",
    "        super().__init__()\n",
    "        self.add(Input(shape=(IMAGE_SIZE,IMAGE_SIZE,3)))\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pool_size = pool_size\n",
    "        self.first_filters = first_filters\n",
    "        self.second_filters = second_filters\n",
    "\n",
    "    def create_cnn(self):\n",
    "        self.add(Conv2D(self.first_filters, self.kernel_size, activation='relu', padding='same', input_shape=(IMAGE_SIZE,IMAGE_SIZE,3)))\n",
    "        self.add(MaxPool2D(pool_size=self.pool_size))\n",
    "        self.add(Conv2D(self.second_filters, self.kernel_size, activation='relu', padding='same'))\n",
    "        self.add(MaxPool2D(pool_size=self.pool_size))\n",
    "\n",
    "        # layers replacing the dense layers\n",
    "        self.add(Conv2D(self.second_filters, (6,6), activation='relu', padding='valid'))\n",
    "        self.add(Conv2D(1, (1,1), activation='sigmoid', padding='same'))\n",
    "        self.add(GlobalAveragePooling2D())\n",
    "\n",
    "    def compile_cnn(self):\n",
    "        self.compile(SGD(learning_rate=0.01, momentum=0.95), loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    def create_autoencoder(self):\n",
    "        # Encoder\n",
    "        self.add(Conv2D(self.first_filters, self.kernel_size, activation='relu', padding='same'))\n",
    "        self.add(MaxPool2D(self.pool_size, padding='same'))\n",
    "        self.add(Conv2D(self.second_filters, self.kernel_size, activation='relu', padding='same'))\n",
    "        self.add(MaxPool2D(self.pool_size, padding='same'))\n",
    "\n",
    "        # Decoder\n",
    "        self.add(Conv2D(self.second_filters, self.kernel_size, activation='relu', padding='same'))\n",
    "        self.add(UpSampling2D(self.pool_size))\n",
    "        self.add(Conv2D(self.first_filters, self.kernel_size, activation='relu', padding='same'))\n",
    "        self.add(UpSampling2D(self.pool_size))\n",
    "        self.add(Conv2D(3, self.kernel_size, activation='sigmoid', padding='same'))\n",
    "\n",
    "    def compile_autoencoder(self):\n",
    "        self.compile(Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the CNN model on regular data and evaluating the model\n",
    "\n",
    "In the next part, an instance of the `model_architecture` class is created for the CNN model. A kernel size of (3,3) and a pooling size of (4,4) is used. After that, the training phase will be initiated. This is followed by a ROC curve analysis of the trained CNN model. The training is done with the regular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = model_architecture(kernel_size=(3,3), pool_size=(4,4), first_filters=32, second_filters=64)\n",
    "model_cnn.create_cnn()\n",
    "model_cnn.compile_cnn()\n",
    "model_cnn._name = 'cnn'\n",
    "\n",
    "model_cnn.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and weights\n",
    "model_name = 'cnn'\n",
    "model_filepath = model_name + '.json'\n",
    "weights_filepath = model_name + '_weights.hdf5'\n",
    "\n",
    "model_json = model_cnn.to_json() # serialize model to JSON\n",
    "with open(model_filepath, 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Define the model checkpoint and Tensorboard callbacks\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(os.path.join('logs', model_name))\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "# Train the model\n",
    "train_steps = train_gen.n//train_gen.batch_size\n",
    "val_steps = val_gen.n//val_gen.batch_size\n",
    "\n",
    "history = model_cnn.fit(train_gen, steps_per_epoch=train_steps,\n",
    "                        validation_data=val_gen,\n",
    "                        validation_steps=val_steps,\n",
    "                        epochs=3,\n",
    "                        callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting labels and predictions on validation set\n",
    "val_true = val_gen.classes\n",
    "val_probs = model_cnn.predict(val_gen, steps=val_steps)\n",
    "\n",
    "# Calculating false positive rate (fpr), true positive rate (tpr) and AUC\n",
    "fpr, tpr, thresholds = roc_curve(val_true, val_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Generate ROC curve\n",
    "roc = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n",
    "roc.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating the autoencoder model\n",
    "\n",
    "First, we need to construct new data generators. The training process of the autoencoder is unsupervised so the class mode of the data generators should be set to `input`. With these new generators, the autoencoder can be trained effectively. A new instance of the `model_architecture` class is created for the autoencoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Constructing the data generators for unsupervised learning for autoencoder training\n",
    "train_gen_ae, val_gen_ae = get_pcam_generators('../data', \n",
    "                                               train_batch_size=16, \n",
    "                                               val_batch_size=16, \n",
    "                                               class_mode='input') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 96, 96, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 48, 48, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 48, 48, 16)        4624      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 24, 24, 16)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 24, 24, 16)        2320      \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2  (None, 48, 48, 16)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 48, 48, 32)        4640      \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSamplin  (None, 96, 96, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 96, 96, 3)         867       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13347 (52.14 KB)\n",
      "Trainable params: 13347 (52.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_ae = model_architecture(kernel_size=(3,3), pool_size=(2,2), first_filters=32, second_filters=16)\n",
    "model_ae.create_autoencoder()\n",
    "model_ae.compile_autoencoder()\n",
    "model_ae._name = 'Autoencoder'\n",
    "\n",
    "model_ae.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2250/2250 [==============================] - ETA: 0s - loss: 0.0154\n",
      "Epoch 1: val_loss improved from inf to 0.01169, saving model to autoencoder_weights.hdf5\n",
      "2250/2250 [==============================] - 456s 202ms/step - loss: 0.0154 - val_loss: 0.0117\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20212077\\.conda\\envs\\8p361\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1124/2250 [=============>................] - ETA: 3:20 - loss: 0.0111"
     ]
    }
   ],
   "source": [
    "# Save the model and weights\n",
    "model_name = 'autoencoder'\n",
    "model_filepath = model_name + '.json'\n",
    "weights_filepath = model_name + '_weights.hdf5'\n",
    "\n",
    "model_json = model_ae.to_json() # serialize model to JSON\n",
    "with open(model_filepath, 'w') as json_file:\n",
    "    json_file.write(model_json) \n",
    "\n",
    "# Define the model checkpoint and Tensorboard callbacks\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(os.path.join('logs', model_name))\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "# Train the model\n",
    "train_steps_ae = train_gen_ae.n//train_gen_ae.batch_size//4\n",
    "val_steps_ae = val_gen_ae.n//val_gen_ae.batch_size//4\n",
    "\n",
    "history = model_ae.fit(train_gen_ae, steps_per_epoch=train_steps_ae, \n",
    "                       validation_data=val_gen_ae,\n",
    "                       validation_steps=val_steps_ae,\n",
    "                       epochs=3,\n",
    "                       callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the output of the trained CAE model. The output of the CAE model is used as augmented dataset in the upcoming steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a prediction on the validation set\n",
    "img_batch = train_gen_ae[0][1] # [batch][class][image_nr]\n",
    "predict_test = model_ae.predict(img_batch) \n",
    "image_nr = 1\n",
    "\n",
    "fig,ax = plt.subplots(1,2)\n",
    "ax[0].imshow(img_batch[image_nr])\n",
    "ax[0].set_title('Original image')\n",
    "ax[1].imshow(predict_test[image_nr])\n",
    "ax[1].set_title('Reconstructed image');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training CNN with CAE preprocessing function\n",
    "\n",
    "The generators for the augmented data are initialized with the CAE model as preprocessing function. The `model_architecture` class is used again to create a new instance for this model. This model is trained on the augmented dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written by Constantijn\n",
    "def model_transform(image):\n",
    "     image = utils.img_to_array(image)\n",
    "     image = np.array([image]) # Convert into a single batch\n",
    "     image_prediction = model_ae.predict(image)\n",
    "\n",
    "     return image_prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the data generators for the augmented dataset  \n",
    "train_gen_aug, val_gen_aug = get_pcam_generators('../data', \n",
    "                                                 class_mode='binary', \n",
    "                                                 prep_function=model_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_aug = model_architecture(kernel_size=(3,3), pool_size=(4,4), first_filters=32, second_filters=64)\n",
    "model_cnn_aug.create_cnn()\n",
    "model_cnn_aug.compile_cnn()\n",
    "model_cnn_aug._name = 'cnn_aug'\n",
    "\n",
    "model_cnn_aug.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and weights\n",
    "model_name = 'cnn_aug'\n",
    "model_filepath = model_name + '.json'\n",
    "weights_filepath = model_name + '_weights.hdf5'\n",
    "\n",
    "model_json = model_cnn_aug.to_json() # serialize model to JSON\n",
    "with open(model_filepath, 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Define the model checkpoint and Tensorboard callbacks\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(os.path.join('logs', model_name))\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "# Train the model\n",
    "train_steps_cnn_aug = train_gen_aug.n//train_gen_aug.batch_size\n",
    "val_steps_cnn_aug = val_gen_aug.n//val_gen_aug.batch_size\n",
    "\n",
    "history = model_cnn_aug.fit(train_gen_aug, steps_per_epoch=train_steps_cnn_aug,\n",
    "                            validation_data=val_gen_aug,\n",
    "                            validation_steps=val_steps_cnn_aug,\n",
    "                            epochs=1,\n",
    "                            callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting labels and predictions on validation set\n",
    "val_true = val_gen_aug.classes\n",
    "val_probs = model_cnn_aug.predict(val_gen_aug, steps=val_steps_cnn_aug)\n",
    "\n",
    "# Calculating false positive rate (fpr), true positive rate (tpr) and AUC\n",
    "fpr, tpr, thresholds = roc_curve(val_true, val_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Generate ROC curve\n",
    "roc = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n",
    "roc.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
